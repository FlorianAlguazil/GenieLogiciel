<article>
	<preamble>Torres.pdf</preamble>
	<title>Summary Evaluation with and without References</title>
	<authors>Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales</authors>
	<abstract>Abstract—We study a new content-based method for the evaluation of text summarization systems without human models which is used to produce system rankings. The research is carried out using a new content-based evaluation framework called FRESA to compute a variety of divergences among probability distributions. We apply our comparison framework to various well-established content-based evaluation measures in text summarization such as COVERAGE, RESPONSIVENESS, PYRAMIDS and ROUGE studying their associations in various text summarization tasks including generic multi-document summarization in English and French, focus-based multi-document summarization in English and generic single-document summarization in French and Spanish. Index Terms—Text summarization evaluation, content-based evaluation measures, divergences. </abstract>
	<biblio>Summary Evaluation with and without Referencesand ranks sentences according to their Jensen-Shannon– a lead-based summarization system that selects the lead– a random-based summarization system that selects– Open Text Summarizer [23], a multi-lingual summarizer– commercial systems: Word, SSSummarizer8and Copernic10C. Evaluation Measuresthe content of the summaries are used in our experiments:peer summary conveys the same information as a modelThis measure is used as indicated in equation 3 using– RESPONSIVENESS ranks summaries in a 5-point scaleinformation need [2]. It is used in focused-basedin equation 4 since a human judges the summaryquestion). RESPONSIVENESS was used in DUC and TAC– PYRAMIDS [11] is a content assessment measure whichcontent units in a set of model summaries. Thisreferences or models. PYRAMIDS is the adopted metricFor DUC and TAC datasets the values of these measures areautomatic evaluation measures in our experiments:account n-grams as units of content for comparing peer[10] is as follows:PPPP(5)model (human) summaries, countmatch is the number ofof n-grams in the model summaries. For the experiments9http://www.pertinence.netpresented here we used uni-grams, 2-grams, and the skipROUGE-2 and ROUGE-SU4). ROUGE is used to compareframework (as indicated in equation 3).is implemented in our FRESA package with the followingof words w.CTN(wif w ∈ Sw +δ(6)text T and Q is the probability distribution of words wsummary N = NT +NS, B = 1.5|V |, CTof words in the text and CSthe summary. For smoothing the summary’s probabilitiesother smoothing approaches (e.g. Good-Turing [24], thatpackage11the experiments reported here. Following the ROUGEand skip n-grams computing divergences such as J Sskip n-grams of ROUGE-SU4), and J SM which is anpeer summary to its source document(s) in our frameworkof multiple documents, these are concatenated (in theprobabilities are computed.We first replicated the experiments presented in [12] toresults compatible with that work. We used the TAC’08ROUGE measures for each peer summary. We producedcompared to rankings produced using the manual PYRAMIDScomputed among the different rankings. The results areamong PYRAMIDS, RESPONSIVENESS and J S. We alsoSpearman correlation, not shown in the table) in this task andThen, we experimented with data from DUC’04, TAC’0811http://search.cpan.org/∼bjoernw/Statistics-Smoothing-SGT-2.1.2/16SPEARMAN CORRELATION OF CONTENT-BASED MEASURES IN TAC’08Mesure PYRAMIDS p-value RESPONSIVENESS p-valueJ S 0.85 p < 0.005 0.74 p < 0.005of the fact that the experiments for French and Spanish corporafor English, results are still quite significant. For DUC’04,tasks 2 and 5 and we used J S, ROUGE, COVERAGE andvarious Spearman’s rank correlation values for DUC’04 areFor task 2, we have verified a strong correlation betweenJ S and COVERAGE is weak, and that between J S andAlthough the Opinion Summarization (OS) task is a newissue, we have decided to compare J S rankings with thoseSpearman’s correlation values are listed in Table IV. As it canboth PYRAMIDS and RESPONSIVENESS. Correlation betweentask (0.71 Spearman’s correlation value).in Spanish and French, we have run 11 multi-lingualhave run 12 systems. In both cases, we have producedof the authors’ provided abstracts. We have then computed J Sthe measure’s values for each system. These averages wereSpearman’s correlations for all pairs of rankings.show medium to strong correlation between the J S measuresuni-grams has lower correlation than J Ss which use n-gramsgeneric multi-document summarization in French, in thissingle-document summarization in French, a result which maysummarization.The departing point for our inquiry into text summarizationevaluation metrics that do not rely on human models but thathave some positive and some negative results regarding theWe have verified that in both generic muti-documentsummarization in English correlation among measuresand ROUGE) and a measure that does not use modelsamong the same measures is weak for summarization ofblogs. We believe that in these cases content-based measuressummarization task (i.e. text-based representation, description)determinant factor in the selection of content for the summary.summarization confirm a strong correlation among thethat ROUGE is in general the chosen framework forsummarization.only have one model summary to compare with the peers.written by the authors. As the experiments in [26] show, themedical domain) adopt similar strategies to summarize theirfor their summaries. Previous studies have shown that authorthese abstracts are ideal candidates for comparison purposes.can be taken as reference for summaries evaluation. It is worthused in summarization evaluation [28]. In the French corpuscase.This paper has presented a series of experiments insummaries for comparison purposes. We have carried outdrawing a clearer picture of tasks where the measures could– We have shown that if we are only interested in rankingautomatic summaries, there are tasks were models couldthe J S measure obtaining reliable rankings. However,by full-documents is not always advisable. We have17 Polibits (42) 2010TABLE IIMesure COVERAGE p-valueJ S 0.68 p < 0.0025SPEARMAN ρ OF CONTENT-BASED MEASURES IN DUC’04 TASK 5ROUGE-2 0.78 p < 0.001 0.44 p < 0.05TABLE IVMesure PYRAMIDS p-value RESPONSIVENESS p-valueTABLE VMesure ROUGE-1 p-value ROUGE-2 p-value ROUGE-SU4 p-valueJ S2 0.88 p < 0.001 0.80 p < 0.002 0.81 p < 0.005J SM 0.82 p < 0.005 0.71 p < 0.020 0.71 p < 0.010complex summarization tasks such as the summarizationopinions.Spanish and French which show positive medium toROUGE and divergence measures that do not use the– We have also presented a new framework, FRESA, forFollowing the ROUGE approach, FRESA package usedivergences. This framework will be available to theAlthough we have made a number of contributions, this paperorder to verify correlation between ROUGE and J S, in thelanguages such as Portuguese and Chinesse for which weplan to apply FRESA to the rest of the DUC and TACAs a novel idea, we contemplate the possibility of adapting[29], which, to our knowledge, does not have an efficientan automatically-compressed sentence taking the completea representation of the task/topic in the calculation ofdependent on the existence of references.campaign INEX’2010 (http://www.inex.otago.ac.nz/tracks/qa/to answer a question by extraction and agglomeration ofto those for which we have found a high correlationhuman intervention. Moreover, the J S calculation will berelevant passages from Wikipedia. FRESA will be used tomulti-document summarizer guided by a query, the searchanswering systems.We are grateful to the Programa Ramón y Cajal frompartially supported by: a postdoctoral grant from the NationalPlan of Scientific Research, Development and Innovationresearch project CONACyT, number 82050, and the researchMéxico), number IN403108.18SPEARMAN ρ OF CONTENT-BASED MEASURES WITH ROUGE IN THE PISTES CORPUS (FRENCH)J S 0.70 p < 0.050 0.73 p < 0.05 0.73 p < 0.500J S4 0.83 p < 0.020 0.76 p < 0.05 0.76 p < 0.050TABLE VIIMeasure ROUGE-1 p-value ROUGE-2 p-value ROUGE-SU4 p-valueJ S2 0.800 p < 0.005 0.590 p < 0.05 0.680 p < 0.02J SM 0.850 p < 0.002 0.640 p < 0.05 0.740 p < 0.01[1] I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, andLanguage Engineering, vol. 8, no. 1, pp. 43–68, 2002.no. 6, pp. 1506–1520, 2007.USA: NIST, November 17-19 2008.Processing Systems, An Analysis and Review, ser. Lecture Notes in[5] R. L. Donaway, K. W. Drummey, and L. A. Mather, “A comparison ofWorkshop on Automatic Summarization, 2000, pp. 69–78.of Summaries in a Cross-lingual Environment using Content-based[7] D. R. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer, H. Qi, A. Çelebi,summarization,” in ACL’03, 2003, pp. 375–382.for automatic evaluation of machine translation,” in ACL’02, 2002, pp.[9] K. Pastra and H. Saggion, “Colouring summaries BLEU,” in Evaluation14 April 2003.Summaries,” in Text Summarization Branches Out: ACL-04 Workshop,[11] A. Nenkova and R. J. Passonneau, “Evaluating Content Selection in145–152.in Summarization without Human Models,” in Empirical Methods in[Online]. Available: http://www.aclweb.org/anthology/D/D09/D09-1032Transactions on Information Theory, vol. 37, no. 145-151, 1991.N-gram Co-occurrence Statistics,” in HLT-NAACL. Morristown, NJ,[15] C.-Y. Lin, G. Cao, J. Gao, and J.-Y. Nie, “An information-theoreticMorristown, USA, 2006, pp. 463–470.Math. Stat., vol. 22, no. 1, pp. 79–86, 1951.Sciences. McGraw-Hill, 1998.“A French Human Reference Corpus for multi-documentsMalta, 2010, p. In press.of Associative Memories: performants applications of Enertex algorithm861–871.“Condensés de textes par des méthodes numériques,” in JADT’02, vol. 2,[21] J. Vivaldi, I. da Cunha, J.-M. Torres-Moreno, and P. Velázquez-Morales,resources,” in LREC’10, vol. 2, Malta, 2010, p. In press.appliqué au résumé automatique de texte,” in JADT’10. Rome, 2010,[23] V. Yatsko and T. Vishnyakov, “A method for evaluating modernand Mathematical Linguistics, vol. 41, no. 3, pp. 93–103, 2007.Language Processing. Cambridge, Massachusetts: The MIT Press,[25] K. Spärck Jones, “Automatic summarising: The state of the art,” IPM,[26] I. da Cunha, L. Wanner, and M. T. Cabré, “Summarization of specializedno. 2, pp. 249–286, 2007.Student Research Workshop. Toulouse, France: Association for[28] K. Owkzarzak and H. T. Dang, “Evaluation of automatic summaries:Singapore, August 2009, pp. 23–30.Sentence compression,” in Proceedings of the National Conference onAAAI Press; MIT Press; 1999, 2000, pp. 703–710.19 Polibits (42) 2010</biblio>
</article>
