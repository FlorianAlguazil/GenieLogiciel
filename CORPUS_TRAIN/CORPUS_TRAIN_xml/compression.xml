<article>
	<preamble>compression.pdf</preamble>
	<title>Multi-Candidate Reduction: Sentence Compression as a Tool for Document Summarization Tasks∗</title>
	<authors>David Zajic1, Bonnie J. Dorr1, Jimmy Lin1, Richard Schwartz21University of MarylandCollege Park, Maryland, USAdmzajic@cs.umd.edu, bonnie@umiacs.umd.edu, jimmylin@umd.edu2BBN Technologies9861 Broken Land ParkwayColumbia, MD 21046schwartz@bbn.com</authors>
	<abstract>Abstract This article examines the application of two single-document sentence compression techniques to the problem of multi-document summarization—a “parse-and-trim” approach and a statisti- cal noisy-channel approach. We introduce the Multi-Candidate Reduction (MCR) framework for multi-document summarization, in which many compressed candidates are generated for each source sentence. These candidates are then selected for inclusion in the final summary based on a com- bination of static and dynamic features. Evaluations demonstrate that sentence compression is a valuable component of a larger multi-document summarization framework. Keywords: headline generation, summarization, parse-and-trim, Hidden Markov Model PACS: Artificial intelligence, 07.05.Mh; Computer science and technology, 89.20.Ff; Spoken lan- guages, processing of, 43.71.Sy </abstract>
	<biblio>as redundancy, chronological order, and source preferences. MCR differs in that multiple variants ofMinimization of redundancy is an important element of a multi-document summarization system.ranking documents returned by an information retrieval system so that the front of the ranked list willsummarization. MCR borrows the ranking approach of MMR, but uses a different set of features. Liketraining data.news (Muresan et al., 2001; Clarke and Lapata, 2006). Within the MCR framework, we discuss thetranscriptions, email threads, and text in foreign language.Our general approach to the generation of a summary from a single document is to produce a headlineand corresponding headline:Department has decided that airline pilots will not be allowed to have guns in the(ii) Pilots not allowed to have guns in cockpits.This basic approach has been realized in two ways. The first, Trimmer, uses a linguistically-is met. Topiary is a variant of Trimmer that combines fluent text from a compressed sentence withthe most likely headline for a given story. The remainder of this section will present Trimmer, Topiary,3.1 Trimmerthe parse tree of a sentence using linguistically-motivated rules until a length threshold has been met.very short summary, or headline. This idea is implemented in our Trimmer system, which can leverageCharniak’s parser (Charniak, 2000).study, which compared the relative prevalence of certain constructions in human-written summariesthe TIPSTER corpus (Harman and Liberman, 1993) dated January 1, 1989. The 218 summaries andparser produced 957 noun phrases (NP nodes in the parse trees) and 315 clauses (S nodes in the parse316 clauses.counted.phrases were counted. Children of the root S node that occur to the left of the first NP arehas gone down” is a prototypical example of a preposed adjunct.counted. Trailing constituents are those not designated as an argument of a verb phrase.• At the NP level, determiners and relative clauses were counted.are shown in Table 1. The results of this analysis illuminated the opportunities for trimming con-5Level Phenomenon Summary Lead Sentenceconjoined S 1/218 0.5% 3/73 4%Clause temporal expression 5/315 1.5% 77/316 24%trailing SBAR 24/315 8% 49/316 16%determiner 31/957 3% 205/817 25%3.1.1 Trimmer Algorithm1. Remove temporal expressions3. Remove preposed adjuncts5. Remove conjunctions7. Remove complementizer that9. Remove PPs that do not contain named entities11. Remove SBARSs13. Remove SBARs15. Remove all PPsTemporal expressions—although certainly not content-free—are not usually vital for summarizingelimination of temporal expressions (Step 1) allow other more important details to remain in the length-(Bikel et al., 1999) for removal of temporalThe determiner rule (Step 4) removes leaf nodes that are assigned the part-of-speech tag DT and6articles is expendable in summaries, even though this makes the summaries ungrammatical for generaland (3), taken from the New York Times website on September 27, 2006, illustrate this phenomenon.(2) The Gotti Case Ends With a Mistrial for the Third Time in a YearStatefor the sentence. Such nodes will be referred to as Root S nodes. A node in a tree is a Root S node if itheadlines we studied always conform to this rule. It has been adopted as a constraint in the Trimmerexample of this rule application is shown in (4). The boldfaced material in the parse is retained and(4) (i) Input: Rebels agreed to talks with government officials, international observers said Tuesday.tional observers said Tuesday.]When the parser produces a usable parse tree, this rule selects a valid starting point for compression.(5) (i) Parse: [S[SBAR What started as a local controversy][VP has evolved into an(ii) Parse: [NP[NP Bangladesh][CC and][NP[NP India][VP signed a water sharingIn (5i), an S exists, but it does not conform to the requirements of the Root S rule because it doesS, ignoring the constraints on the children. In (5ii), no S is present in the parse. This problem isoccur infrequently—only 6% of the sentences in the DUC-2003 evaluation data exhibit these problems,The motivation for removing preposed adjuncts (Step 3) is that all of the human-generated headlinesthe first NP (the subject) under the Root S chosen in Step 2; the preamble of a sentence consists of its(6) (i) Input: According to a now finalized blueprint described by U.S. officials and other sources,Iraq.sources], [Det the] Bush administration plans to take complete, unilateral control7(iii) Output: Bush administration plans to take complete unilateral control of post-SaddamThe remaining steps of the algorithm remove linguistically peripheral material through successivealgorithm corresponds to the application of one of the rules. Trimmer first finds the pool of nodes inremaining node in the pool until the length threshold is reached or the pool is exhausted. After a ruleIn the case of a conjunction with two children (Step 5), one of the children will be removed. If theThis rule is illustrated by the following examples, where the italicized text is trimmed.prestigious auction house apologized and bought it back.24 people but said the attack should not derail the recent land-for-security deal between IsraelThe modal verb rule (Step 6) applies to verb phrases in which the head is a modal verb and theare removed. Sentences (9) and (10) show examples of this rule application. Note that although inthe loss of the modality is preferable to the loss of other content information.(10) Agents may have fired potentially flammable tear gas cannisters.Sentence (11) shows an example in which two complementizers can be removed.The XP-over-XP rule (Step 8) is a linguistic generalization that allows a single rule to cover twoIn constructions of the form [XP [XP ...] ...], the other children of the higher XP are removed. Noteclauses (as in Sentence (12)) and appositives (as in Sentence (13)).some relief.Center for Molecular and Behavioral Neuroscience in Newark, studied DNA of dozens of membersThe rules that remove prepositional phrases and subordinate clauses (Steps 9 through 15) areare no other types of rules to apply. Moreover, these rules are applied with a backoff option to avoidrule (Step 11). If the desired sentence length has not been reached, the system reverts to the parse tree13). If the desired length still has not been reached, the PP rule is applied again (Steps 14 and 15).able to remove smaller fragments before larger ones and prepositional phrases tend to be smaller thanconstituents (PPs), but if this cannot be accomplished, the system restores the smaller constituents,risk of removing prepositional phrases that contain important information, BBN’s IdentiFinder is used3.1.2 Use of BBN’s IdentiFinder in Trimmerdeletion of PPs containing named entities. The elimination of temporal expressions (Step 1) is a two-...] and [NP [X]] where X is tagged as part of a temporal expression. The following examples illustrate(14) (i) Input: The State Department on Friday lifted the ban it had imposed on foreign fliers.[Det the] ban it had imposed on foreign fliers.]](15) (i) Input: An international relief agency announced Wednesday that it is withdrawing from(ii) Parse: [S [NP [Det An] international relief agency][VP announced [NP [NNP Wednes-(iii) Output: International relief agency announced that it is withdrawing from North Korea.removed during the first round of PP removal (Step 9). However, prepositional phrases containingwe should remove a smaller constituent before removing a larger constituent that subsumes it. Sentence(16) The commercial fishing restrictions in Washington will not be lifted [SBAR unless the salmonIf the PP rule were not sensitive to named entities, the PP in the Columbia River would be thethis PP provides an important piece of information: the location of the salmon population. The rule inThis concludes an overview of the Trimmer rules and our syntactic sentence compression algorithm.Multiple compressions can be generated by setting the length limit to be very small and storing the statemultiple compressed candidates generated by Trimmer are used as a component of a multi-document193.2 Topiarycalled Topiary. This system combines Trimmer with a topic discovery approach (described next) toThe Trimmer algorithm is constrained to build a headline from a single sentence. However, it isinformation can be spread over multiple sentences, linked by anaphora or ellipsis. In addition, theOn the other hand, approaches that construct headlines from lists of topic terms (Lewis, 1999;described below—rarely generates any topic terms that are verbs. Thus, topic lists are good at indi-we need both fluent text to tell what happened and topic terms to provide context.OnTopic (Schwartz et al., 1997) uses an HMM to assign topics to a document; topic models are derivedor language. UTD (Sista et al., 2002) was developed to overcome this limitation: it takes as input aThe UTD algorithm has several stages. First, it analyzes the corpus to find multi-word sequencesdescription length criterion that detects phrases that occur frequently relative to the individual words.the text as additional tokens. They are also likely to be chosen as potential topic names. In the secondtopic names that occur as high-content terms in at least four different documents are kept. The thirdprocedure of BBN’s OnTopic system is used to determine which words in the documents often signifylikely topics for each document, which is equivalent to assigning the name of the topic model to thein the document text.were usually meaningful and that the topic assignment was about as good as when the topics werelanguages and shown the same behavior. Since UTD is unsupervised, it can run equally well on a newThe topic list in (17) was generated by UTD and OnTopic for a story about the FBI investigation(17) BIN LADEN, EMBASSY, BOMBING, POLICE OFFICIALS, PRISON, HOUSE, FIRE, KA-Topiary uses UTD to generate topic terms for the collection of documents to be summarized andterms and sentence compressions are combined to form Topiary summaries.As each Trimmer rule is applied to a sentence, the resulting state of the sentence is stored as athat there is room to prepend the highest scoring non-redundant topic term. Suppose the highest“terrorism”, the length threshold is lowered by 10 characters: 9 characters for the topic and 1 charactercontain the word “terrorism”, If there is no such candidate, i.e., all the trimmed variants contain theselect the longest trimmed variant under 70 characters that does not contain the word “bomb”. Afterspace remains under the threshold. Additional topic words are added between the first topic word andThis process results in a headline that contains one or more main topics about the story and aa fully fluent sentence and compensates for the fact that the information content from the topic andAs examples, sentences (18) and (19) are the outputs of Trimmer and Topiary, respectively, for the(18) FBI agents this week began questioning relatives of the victimsBy combining topics and parse-and-trim compression, Topiary achieved the highest score on the3.3 HMM Hedge(the story) as the result of unobserved data (headlines) that have been distorted by transmissionwords. The model is biased by parameters to make the resulting headlines more like Headlinese, theFormally, we consider a story S to be a sequence of N words. We want to find a headline H, aargmaxHP(H|S)probabilities that are easier to compute, using Bayes’ rule:P(S)denominator of the above expression can be omitted. Thus we wish to find:Let H be a headline consisting of words h1, h2, ..., hn. Let the special symbols start and end representHeadlinese:11The bigram probabilities of the words in the headline language were computed from a corpus ofcontain 2,848,194 words from a vocabulary of 88,627 distinct words.words to H. Let G be the non-headline words added by the channel to the headline: g1, g2, ..., gm. For1. We estimate P(S|H), the probability that the channel added non-headline words G to headline Hto as the general language, in contrast to the headline language. Let Pgl(g) be the probability ofis transmitted through the channel as story word h.= Pgl(g1)Pgl(g2)...Pgl(gm)AP news stories in the TIPSTER corpus. The stories contain 135,150,288 words from a vocabulary ofThe process by which the noisy channel generates a story from a headline can be represented by aeach state probabilistically emits a string. The simplest HMM for generating headlines consists of twowords in the story.the previously emitted headline word. If we did not constrain headline words to actually occur in thewords are chosen from the story words, it is sufficient to have an H state for each story word. For anycorresponding G state for each H state, and a state Gstart that emits words that occur before the firststate remembers which word was emitted by its H state and can emit any word in the story language.in the correct order. In practice the HMM is constructed with states for only the first N words of theIn example (1i), given earlier, the H states will emit the words in bold (pilots, not, allowed, to,between the H and G states as needed to generate the words of the story. In the current example,headline given in example (1ii) corresponds to the following sequence of states: Start, Gstart 17 times,only one that could generate the story in (1i). Other possibilities are:(ii) Months of the terrorist has to have cockpits.Limiting consideration of headline words to the early part of the story is justified in Dorr et al. (2003a) where itselecting the window of story words are possible and will be explored in future research.The subscript of a G state indicates the H state it is associated with, not the story word it emits. In the example,12Although (20i) and (20ii) are possible headlines for (1i), the conditional probability of (20ii) given (1i)The Viterbi algorithm (Viterbi, 1967) is used to select the most likely headline for a given story.from 5 to 15. Multiple backpointers are used so that we can find the n most likely headlines at eachHMM Hedge is enhanced by three additional decoding parameters to help the system choose outputsof these biases changes the score produced by the decoder from a probability to a relative desirabilitytrial and error. A logical extension to this work would be to learn the best setting of these biases, e.g.,The position bias favors headlines that include words near the front of the story. This reflects ourof the story. The initial position bias p is a positive number less than one. The story word in thebias is added to the desirability score. Thus, words near the front of the story carry less of a positioninterest and sports stories, which may start with a hook to get the reader’s attention, rather than aWe also observed that human-constructed headlines tend to contain contiguous blocks of storystring bias is used to favor “clumpiness”. i.e., the tendency to generate headlines composed of clumpstransition from an H state to its associated G state. With high clump biases, the system will favorThe gap bias is used to disfavor headline “gappiness”, i.e., large gaps of non-headline words in theby selecting widely spaced words, we observed that HMM Hedge was more likely to combine unrelatedof a sequence of non-headline words in the story, a gap bias is applied that increases with the size ofas a penalty for spending too much time in one G state. With high gap biases, the system will favorOne characteristic difference between newspaper headline text and newspaper story text is thatverbs occur more rarely in the headline language than in the general language. HMM Hedge mimics thisstory verbs. Morphological variation for verbs is achieved by creating an H state for each availableHMM Hedge can generate a headline in which proposes is the unobserved headline word that emits the(21) (i) A group has proposed awarding $1 million in every general election to one randomly chosen(ii) Group proposes awarding $1 million to randomly chosen voter.that do not contain at least one verb, no matter how desirable the decoding is.to sentence boundaries, it is also possible to use HMM as a single sentence compressor by limiting theAlso, as we will see shortly, multiple alternative compressions of a sentence may be generated withstory words and can be constrained to consider only paths that include a specific number of H states,55 compressions for each sentence by computing the five best headlines at each length, from 5 to 15summarization system.The sentence compression tools we developed for single-document summarization have been incor-produces a textual summary from a collection of relevant documents in three steps. First, sentencesthe front of the stories, so we select the first five sentences of each document for compression. Second,pool of candidates for inclusion in the summary. Finally, a sentence selector constructs the summarysummary reaches a desired length.data to maximize the Rouge-2 recall score (Lin and Hovy, 2003), using Rouge version 1.5.5. Aof dozen news stories. These summaries may be query-focused, in the sense that the summaries shoulddocuments is desired.and Goldstein, 1998), an approach that attempts to balance relevance and anti-redundancy. In MCR’sin the summary. Features that contribute to a candidate’s score can be divided into two types: dynamicAfter a candidate is added to the summary, the dynamic features are re-computed, and the candidatesof candidates in the summary is the same as the order in which they were selected for inclusion. The4.1 Static Featuressummary construction:• Sentence Relevance. The relevance score of the sentence to the query.• Sentence Centrality. The centrality score of the sentence to the topic cluster.• Scores from the Compression Modules:stances applied to produce the candidate.candidate.for information retrieval tasks, to compute relevance and centrality scores for each compressed candi-the relevance score between the document containing the compressed sentence and the query, the cen-the document containing the compressed sentence and the topic cluster. We define the topic clustera concept that quantifies how similar a piece of text is to all other texts that discuss the same generalto the topic cluster are preferred for inclusion in the final summary.overlapping terms (number of terms shared by the two text segments). Inverse document frequencyThe idf of a term t is defined by log(N/ct), where N is the total number of documents in a particularyear’s worth of LA Times articles. Weighting term overlap by inverse document frequency capturesLucene, a freely-available off-the-shelf information retrieval system, is used to compute the threethe query is computed using Lucene’s built-in similarity function. The centrality score between thedocument comprising the cluster (once again, as computed by Lucene’s built-in similarity function).of the similarity of the particular document with every other document in the cluster. In order toall relevant documents (i.e., the topic cluster) along with a comparable corpus (one year of the LASome features are derived from the sentence compression modules used to generate candidates. Fora source sentence to produce the candidate. The rules are not presumed to be equally effective, so thescore calculated by the decoder, expressed as a negative log.begins and remain fixed throughout the process of summary sentence selection. The next section4.2 Dynamic Featuressummary as sentences are added.4 The dynamic features are:At present the dynamic features are properties of the candidates, calculated with respect to the current summary15• Redundancy. A measure of how similar the sentence is to the current summary.The intuition behind our redundancy measure is that candidates containing words that occur muchto the summary. We imagine that sentences in the summary are generated by the underlying wordappears to have been generated by the summary rather than by the general language, we take it tocandidate of words like earthquake, seismic, and Richter Scale, which have a high likelihood in theTo estimate the extent to which a candidate is more likely to have been generated by a summarythat the probability that a word w occurs in a candidate generated by the summary iswhere D is the summary, C is the general language corpus5, λ is a parameter estimating the probabilityby the general language. We have set λ = 0.3, as a general estimate of the portion of words in a textsummary and the general language corpus:count of w in DP(w|C) =size of Cthe probability that a sentence was generated by the summary, i.e. our redundancy metric, as:YλP(s|D) + (1 − λ)P(s|C)Xlog(λP(s|D) + (1 − λ)P(s|C))every iteration of the sentence selector.We applied our MCR framework to test data from the DUC-2006 evaluation (Dang and Harman, 2006).the New York Times, and the Xinhua News Agency English Service), the system’s task was to createaddressed as future work in Section 7.The documents in the set being summarized are used to estimate the general language model.Actually, preprocessing for redundancy includes stopword removal and applying the Porter Stemmer (Porter, 1980).Narrative Description: Discuss conditions on American Indian reservations or among Nativelegal privileges and problems.a 250-word summary that addressed the information need expressed in the topic. One of the topicwords, so a 250-word summary represents a compression ratio of 0.86%.pression, or no compression. For readability, we use ◦ as a sentence delimiter; this is not part of theers and auxiliary verbs. For example, the first sentence in Figure 3 is a compression of the followingSeeking to get a more accurate count of the country’s American Indian population, thelong-standing feelings of wariness or anger toward the federal government.that appears in the summary. The removal of this material makes the sentence appear more like aIn comparison with Trimmer compressions, HMM compressions are generally less readable and(22) main purpose of reservation to pay American Indians by poverty proposals(23) But the main purpose of the visit—the first to a reservation by a president since Franklinpoverty that Clinton’s own advisers suggested he come up with special proposals geared specifi-Because HMM Hedge uses a bigram model of Headlines, it is unable to capture sentence-levelFor example, the third sentence from the end of Figure 4 seems to say that a court legalized gambling(24) Supreme Court allows legalized gambling Indian reservations(25) Only Monday, the California Supreme Court overturned a ballot measure that would have allowedNevertheless, we can see from the examples that sentence compression allows a summary to include17Seeking to get more accurate count of country’s American Indian population, Census Bureau turning toreservations would get infusion. ◦ Smith and thousands seeking help for substance abuse at Americanto Pine Ridge Reservation for visit with Oglala Sioux nation and to participate in conference on NativeServices on 2.8 million-acre Wind River Reservation, about 100 miles east of Jackson, Wyo. “Then weinto Shinnecock Indian reservation is not welcoming one But main purpose of visit – first to reservationgrinding poverty Clinton’s own advisers suggested he come up with special proposals geared specifically toSidney Harring, professor at City University of New York School of Law and expert on Indian crime andmost forgotten ◦ U.S. citizens: American Indians. ◦ When American Indians began embracing gambling,Reservation started seven-acre community garden with donated land, seeds andDavid Rocchio deputy legal counsel to Vermont Gov. Howard Dean who has been involved in discussionsis not with the benefit casinos bring to tribes ◦ Native Americans living on reservations that maintain 50Smith and thousands like her are seeking help for their substance abuse at the American Indian Communitycrime is one strand in the web of social problems facing urban and reservation Indian communities theIndians who live on reservations of whom 49 percent are unemployed ◦ Powless said the Onondaga peoplecreating tourism destinations that might include Indian culture or setting up a free trade zone at unusedof problems they are watching the Navajo Nation’s legal battle with the federal government ◦ recognizeCourt allows legalized gambling Indian reservations ◦ American Indian reservations tribal colleges rise fasterFigure 4: MCR Summary for DUC-2006 Topic D0601A, using HMM Hedge for sentence compressionturning to tribal leaders and residents on reservations to help overcome long-standing feelings of warinessbillion in federal money for education, health care and law enforcement under President Clinton’s proposedIndian Community House, the largest of a handful of Native American cultural institutions in the Newand to participate in a conference on Native American homeownership and economic development. ◦ saidabout 100 miles east of Jackson, Wyo. “Then we came up with the idea for this community garden, andreservation is not a welcoming one ◦ But the main purpose of the visit – the first to a reservation by agrinding poverty that Clinton’s own advisers suggested he come up with special proposals geared specificallyFigure 5: MCR Summary for DUC-2006 Topic D0601A, with no sentence compressionSentence60 BlockR1 Recall 0.235520.24082)(0.20912-0.210140.21594)(0.24632-R1 Precision 0.218960.22384)(0.18444-0.201830.20722)(0.22567-R1 F 0.224960.22978)(0.19505-0.201790.20718)(0.23373-R2 Recall 0.068380.07155)(0.05848-0.063370.06677)(0.06345-R2 Precision 0.062870.06576)(0.05097-0.062300.06617)(0.05747-R2 F 0.064880.06785)(0.05420-0.060790.06401)(0.05976-Table 2: Rouge scores and 95% confidence intervals for 624 documents from DUC-2003 test set.We tested four single-document summarization systems on the DUC-2003 Task 1 test set:• HMM Hedge using the first 60 words of each document (HMM 60 block)• TopiaryNewswire and the New York Times. The average size of the documents was 3,997 bytes, so a 75-byteAn automatic summarization evaluation tool, Rouge (Lin and Hovy, 2003), was used to evaluatea comparable training corpus, 500 AP Newswire and New York Times articles from the DUC-2004The Rouge results are shown in Table 2. Results show that HMM Hedge 60 scored significantlymeasures. In addition, HMM Hedge Sentence scored significantly higher than Trimmer for the R1We also evaluated Trimmer and HMM Hedge as components in our Multi-Candidate Reductioncompression. All three systems considered the first five sentences of each document and used themaximize Rouge-2 recall on a comparable training corpus, 1,593 Financial Times and Los Angeles20Trimmer HMM Hedge NoR1 Recall 0.293910.30247)(0.26554-0.275760.28430)(0.06332-0.062510.06620)(0.05767-Table 3: Rouge scores and 95% confidence intervals for 50 DUC-2006 test topics, comparing threeRouge-2 Rouge-SU4 BE-HMHigher 1 1 0Range 0.0678-0.0899 0.1238-0.1475 0.0318-0.0508Table 4: Official DUC-2006 Automatic Metrics for our MCR submission (System 32).test data, described in Section 4.3.Results are shown in Table 3. MCR using Trimmer compressions scored significantly higher thandifference among the three systems for Rouge-2.2006 evaluation. This version used Trimmer as the source of sentence compressions. Results showsurprising, since Trimmer aims to produce compressions that are grammatical in Headlinese, ratherevaluation of grammaticality. However, the system did not score significantly lower than any otherresponsiveness asked evaluators to take readability into consideration. In this evaluation, MCR scorednot grammatical in standard English; yet, the content coverage was not significantly different from theNIST computed three “official” automatic evaluation metrics for DUC-2006: Rouge-2, Rouge-metrics, along with numbers of systems that scored significantly higher, significantly lower, or were notnot significantly different from MCR. These results show that the performance of our MCR run was7non-essential stopwords (typical of Headlinese) is an important component of Trimmer-based sentence compression. Forresearch focus. For reporting of official Rouge results on submitted systems we use the community’s accepted Rouge21The evaluation in Table 3 suggests that Trimmer sentence compression is preferable to HMMHowever, HMM Hedge may prove to have value with noisier data, as we discuss in the next section.document summarization, thus validating the ideas behind Multi-Candidate Reduction.We have applied the MCR framework to summarizing different types of texts. In this section we brieflywere designed for summarization of written news. In this genre, the lead sentence is almost alwaysdid not outperform the baseline of simply selecting the first sentence for AP wire “hard” news stories.not have informative lead sentences and will require additional work in finding the best sentence forMCR has also been applied to summarizing transcripts of broadcast news—another input formof story-initial light content sentences, such as “I’m Dan Rather” or “We have an update on the storySuch texts are additionally complicated by a range of problems not encountered in written news:sentence boundary detection and story boundary detection. If word error rate is high, parser failuresattractive, since our language models are more resilient to noisy input.first 75 characters of a document, on the task of creating 75-character headlines for broadcast newsABC, CNN, NBC, Public Radio International, and Voice of America. We used Rouge-1 recall toscored higher than Trimmer. However there were no significant differences among the systems.that share a common topic or were written as responses to each other. This task can essentially beconstraints with respect to the ordering of summary sentences. Noisy data is inherent in this problemmetadata, such as the name of the sender of each included extract help make email summaries easierWe performed an initial evaluation of HMM Hedge and Trimmer as the source of sentence com-corpus for this task consisted of 10 manually constructed email threads from the Enron Corpus (Klimtsystems and the human summarizers. We did not observe a significant difference between the two sys-(one summarizer scored significantly worse than the automatic systems). This application of MCR tothe community needs to develop a clearer understanding of what makes a good email thread summary22Finally, Trimmer and HMM Hedge have been applied to Hindi-English cross-language summariza-Hedge to cross-lingual summarization by using the mechanism developed for morphological variationdetails, see Dorr et al. (2003a).Future work on text summarization under the Multi-Candidate Reduction framework will focus on theselection.of a document for compression remains the best approach. However, for human interest stories or sportscontain important information. Currently, filtering for multi-document summarization also relies onsentences of each document are retained to generate compressed candidates. An interesting area ofcentrality, to move beyond the baseline of selecting the first n sentences. For HMM Hedge, theseCurrently, Trimmer produces multiple compressions by applying rules in a fixed order; the state ofcan be produced by modifying Trimmer rules to operate in order-independent combinations, ratherlarger pools of candidates to choose from. Naturally, different sentence compressions are not the onlyanaphora. Topiary will also be enhanced by using multiple combinations of compressions and topicWe also plan to enrich the candidate selector by taking into account more features of the currentwell as feature weights that change during the progress of summary generation. These extensions williterations. System output can potentially be improved by finer-grained control of this distribution.to length restrictions (e.g., by selecting a final sentence of more appropriate length).of parameter values in HMM Hedge and the sentence selector could lead to significant improvementsthrough Expectation Maximization.ordering into consideration when constructing the summary. Naturally, high-quality summaries shouldto MCR includes includes Conroy et al. (2006), Barzilay et al. (2002), Okazaki et al. (2004), Lap-considerations can be balanced with other important factors such as relevance and anti-redundancy238 Conclusionrization. The framework integrates successful single-document compression techniques that we havesentences should be made available to subsequent processing modules, which may have access to morelector that iteratively builds a summary from compressed variants. Evaluations show that sentenceis both flexible and extensible.This work has been supported, in part, under the GALE program of the Defense Advanced ResearchResearch Projects Agency, BBNT Contract No. 9500006806, and the University of Maryland Jointin this paper are those of the authors and do not necessarily reflect the views of DARPA. The firstwould like to thank Steve, Carissa, and Ryan for their energy enablement. The third author would likeReferencesrecognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 5(2):179–190.In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACLR. Barzilay, N. Elhadad, and K. McKeown. 2002. Inferring strategies for sentence ordering in multi-L. Baum. 1972. An inequality and associated maximization technique in statistical estimation ofS. Bergler, R. Witte, M. Khalife, Z. Li, and F. Rudzicz. 2003. Using knowledge-poor coreferenceWorkshop and Document Understanding Conference (DUC 2003), pages 85–92, Edmonton, Alberta.Learning, 34(1/3):211–231.B. Schiffman, A. Schlaikjer, A. Siddharthan, and S. Siegelman. 2004. Columbia UniversityHLT/NAACL 2004, pages 23–30, Boston, Massachusetts.statistical approach to machine translation. Computational Linguistics, 16(2):79–85.documents and producing summaries. In Proceedings of the 21st Annual International ACM SIGIRMelbourne, Australia.of the North American Chapter of the Association for Computational Linguistics (NAACL 2000),J. Clarke and M. Lapata. 2006. Models for sentence compression: A comparison across domains,ence on Computational Linguistics and 44th Annual Meeting of the Association for ComputationalJ. Conroy, J. Schlesinger, and J. Goldstein. 2005. CLASSY query-based multi-document summariza-2005, Vancouver, Canada.Proceedings of the 2006 Document Understanding Conference (DUC 2006) at HLT/NAACL 2006,D. Cutting, J. Pedersen, and P. Sibun. 1992. A practical part-of-speech tagger. In Proceedings of theHoa Dang and Donna Harman. 2006. Proceedings of the 2006 Document Understanding ConferenceB. Dorr and T. Gaasterland. this special issue, 2007. Exploiting aspectual features and connectingagement.Transactions on Asian Language Information Processing (TALIP), 2(3):270–289.generation. In Proceedings of the HLT-NAACL 2003 Text Summarization Workshop and DocumentT. Dunning. 1994. Statistical identification of language. Technical Report MCCS 94-273, New MexicoT. Euler. 2002. Tailoring text using topic words: Selection and compression. In Proceedings of 13th222, Aix-en-Provence, France.sentence extraction. In Proceedings of ANLP/NAACL 2000 Workshop on Automatic Summariza-D. Harman and M. Liberman. 1993. TIPSTER Complete. Linguistic Data Consortium (LDC),25H. Jing and K. McKeown. 2000. Cut and paste based text summarization. In Proceedings of the First2000), pages 178–185, Seattle, Washington.on Email and Anti-Spam (CEAS), Mountain View, California.Proceedings of the Seventeenth National Conference on Artificial Intelligence (AAAI-2000), Austin,K. Knight and D. Marcu. 2002. Summarization beyond sentence extraction: A probabilistic approachM. Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedings545–552, Barcelona, Spain.tion task. In Proceedings of the 15th Annual International ACM SIGIR Conference on ResearchC.-Y. Lin and E. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statis-Chapter of the Association for Computational Linguistics Annual Meeting (HLT/NAACL 2003),I. Mårdh. 1980. Headlinese: On the Grammar of English Front Page Headlines. Malmo.Natural Language ITL, pages 517–522, Paris, France.information from text. In Proceedings of the First Meeting of the North American Chapter of theS. Muresan, E. Tzoukermann, and J. Klavans. 2001. Combining linguistic and machine learningLanguage Learning (ConLL), pages 290–297, Toulouse, France.dence relation. In Proceedings of the 20th International Conference on Computational LinguisticsM. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.W. Lam, D. Liu, J. Otterbacher, H. Qi, H. Saggion, S. Teufel, M. Topper, A. Winkel, and Z. Zhang.the 4th International Conference on Language Resources and Evaluation (LREC 2004), Lisbon,26R. Schwartz, T. Imai, F. Jubala, L. Nguyen, and J. Makhoul. 1997. A maximum likelihood modeltion Association Conference on Speech Communication and Technology (Eurospeech-97), Rhodes,S. Sista, R. Schwartz, T. Leek, and J. Makhoul. 2002. An algorithm for unsupervised topic discovery(HLT), pages 99–103, San Diego, California.In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACLL. Vanderwende, H. Suzuki, and C. Brockett. 2006. Microsoft Research at DUC2006: Task-focusedument Understanding Conference (DUC 2006) at HLT/NAACL 2006, New York, New York.rithm. IEEE Transactions on Information Theory, 13:260–269.style approaches to headline generation. In Lecture Notes in Computer Science: Advances inSantiago de Compostela, Spain. Springer Berlin / Heidelberg.the 2004 Document Understanding Conference (DUC 2004) at NLT/NAACL 2004, pages 112–119,D. Zajic, B. Dorr, J. Lin, and R. Schwartz. 2005a. UMD/BBN at MSE2005. In Proceedings of theMT and/or Summarization, Ann Arbor, Michigan.multi-document summarization. In Proceedings of the 2005 Document Understanding ConferenceD. Zajic. 2007. Multiple Alternative Sentence Compressions (MASC) as a Tool for Automatic Sum-L. Zhou and E. Hovy. 2003. Headline summarization at ISI. In Proceedings of the HLT-NAACL174–178, Edmonton, Alberta.</biblio>
</article>
